// Copyright 2012 NVIDIA Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

#include <stdio.h>
#include <assert.h>

// Convenience function for checking CUDA runtime API results
// can be wrapped around any runtime API call. No-op in release builds.
inline
cudaError_t checkCuda(cudaError_t result)
{

  if (result != cudaSuccess) {
    fprintf(stderr, "CUDA Runtime Error: %s\n", cudaGetErrorString(result));
    assert(result == cudaSuccess);
  }

  return result;
}

const int TILE_DIM = 32;
const int BLOCK_ROWS = 8;
const int NUM_REPS = 10;

// Check errors and print GB/s
void postprocess(const float *ref, const float *res, int n, float ms)
{
  bool passed = true;
  for (int i = 0; i < n; i++)
    if (res[i] != ref[i]) {
      printf("%d %f %f\n", i, res[i], ref[i]);
      printf("%25s\n", "*** FAILED ***");
      passed = false;
      break;
    }
  if (passed)
    printf("%20.2f\n", 2 * n * sizeof(float) * 1e-6 * NUM_REPS / ms );
}

// simple copy kernel
// Used as reference case representing best effective bandwidth.
__global__ void copy(float *odata, const float *idata)
{
  int x = blockIdx.x * TILE_DIM + threadIdx.x;
  int y = blockIdx.y * TILE_DIM + threadIdx.y;
  int width = gridDim.x * TILE_DIM;

  for (int j = 0; j < TILE_DIM; j+= BLOCK_ROWS)
    odata[(y+j)*width + x] = idata[(y+j)*width + x];
}

// copy kernel using shared memory
// Also used as reference case, demonstrating effect of using shared memory.
__global__ void copySharedMem(float *odata, const float *idata)
{
  __shared__ float tile[TILE_DIM * TILE_DIM];

  int x = blockIdx.x * TILE_DIM + threadIdx.x;
  int y = blockIdx.y * TILE_DIM + threadIdx.y;
  int width = gridDim.x * TILE_DIM;

  for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS)
     tile[(threadIdx.y+j)*TILE_DIM + threadIdx.x] = idata[(y+j)*width + x];

  __syncthreads();

  for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS)
     odata[(y+j)*width + x] = tile[(threadIdx.y+j)*TILE_DIM + threadIdx.x];
}

// naive transpose
// Simplest transpose; doesn't use shared memory.
// Global memory reads are coalesced but writes are not.
__global__ void transposeNaive(float *odata, const float *idata)
{
  int x = blockIdx.x * TILE_DIM + threadIdx.x;
  int y = blockIdx.y * TILE_DIM + threadIdx.y;
  int width = gridDim.x * TILE_DIM;

  for (int j = 0; j < TILE_DIM; j+= BLOCK_ROWS)
    odata[x*width + (y+j)] = idata[(y+j)*width + x];
}

// coalesced transpose
// Uses shared memory to achieve coalesing in both reads and writes
// Tile width == #banks causes shared memory bank conflicts.
__global__ void transposeCoalesced(float *odata, const float *idata)
{
  __shared__ float tile[TILE_DIM][TILE_DIM];

  int x = blockIdx.x * TILE_DIM + threadIdx.x;
  int y = blockIdx.y * TILE_DIM + threadIdx.y;
  int width = gridDim.x * TILE_DIM;

  for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS)
     tile[threadIdx.y+j][threadIdx.x] = idata[(y+j)*width + x];

  __syncthreads();

  x = blockIdx.y * TILE_DIM + threadIdx.x;  // transpose block offset
  y = blockIdx.x * TILE_DIM + threadIdx.y;

  for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS)
     odata[(y+j)*width + x] = tile[threadIdx.x][threadIdx.y + j];
}


// No bank-conflict transpose
// Same as transposeCoalesced except the first tile dimension is padded
// to avoid shared memory bank conflicts.
__global__ void transposeNoBankConflicts(float *odata, const float *idata)
{
  __shared__ float tile[TILE_DIM][TILE_DIM+1];

  int x = blockIdx.x * TILE_DIM + threadIdx.x;
  int y = blockIdx.y * TILE_DIM + threadIdx.y;
  int width = gridDim.x * TILE_DIM;

  for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS)
     tile[threadIdx.y+j][threadIdx.x] = idata[(y+j)*width + x];

  __syncthreads();

  x = blockIdx.y * TILE_DIM + threadIdx.x;  // transpose block offset
  y = blockIdx.x * TILE_DIM + threadIdx.y;

  for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS)
     odata[(y+j)*width + x] = tile[threadIdx.x][threadIdx.y + j];
}


void bandwidth_test(int rows_cols, int argc, char **argv)
{
    const int nx = rows_cols;
    const int ny = rows_cols;
    const int mem_size = nx*ny*sizeof(float);

    dim3 dimGrid(nx/TILE_DIM, ny/TILE_DIM, 1);
    dim3 dimBlock(TILE_DIM, BLOCK_ROWS, 1);

    int devId = 0;
    if (argc > 1) devId = atoi(argv[1]);

    cudaDeviceProp prop;
    checkCuda( cudaGetDeviceProperties(&prop, devId));
    printf("\nDevice : %s\n", prop.name);
    printf("Matrix size: %d %d, Block size: %d %d, Tile size: %d %d\n",
           nx, ny, TILE_DIM, BLOCK_ROWS, TILE_DIM, TILE_DIM);
    printf("dimGrid: %d %d %d. dimBlock: %d %d %d\n",
           dimGrid.x, dimGrid.y, dimGrid.z, dimBlock.x, dimBlock.y, dimBlock.z);

    checkCuda( cudaSetDevice(devId) );

    float *h_idata = (float*)malloc(mem_size);
    float *h_cdata = (float*)malloc(mem_size);
    float *h_tdata = (float*)malloc(mem_size);
    float *gold    = (float*)malloc(mem_size);

    float *d_idata, *d_tdata;
    checkCuda( cudaMalloc(&d_idata, mem_size) );
    checkCuda( cudaMalloc(&d_tdata, mem_size) );

    // check parameters and calculate execution configuration
    if (nx % TILE_DIM || ny % TILE_DIM) {
      printf("nx and ny must be a multiple of TILE_DIM\n");
      goto error_exit;
    }

    if (TILE_DIM % BLOCK_ROWS) {
      printf("TILE_DIM must be a multiple of BLOCK_ROWS\n");
      goto error_exit;
    }

    // host
    for (int j = 0; j < ny; j++)
      for (int i = 0; i < nx; i++)
        h_idata[j*nx + i] = j*nx + i;

    // correct result for error checking
    for (int j = 0; j < ny; j++)
      for (int i = 0; i < nx; i++)
        gold[j*nx + i] = h_idata[i*nx + j];

    // device
    checkCuda( cudaMemcpy(d_idata, h_idata, mem_size, cudaMemcpyHostToDevice) );

    // events for timing
    cudaEvent_t startEvent, stopEvent;
    checkCuda( cudaEventCreate(&startEvent) );
    checkCuda( cudaEventCreate(&stopEvent) );
    float ms;

    // ------------
    // time kernels
    // ------------
    printf("%25s%25s\n", "Routine", "Bandwidth (GB/s)");

    // ----
    // copy
    // ----
    printf("%25s", "copy");
    checkCuda( cudaMemset(d_tdata, 0, mem_size) );
    // warm up
    copy<<<dimGrid, dimBlock>>>(d_tdata, d_idata);
    checkCuda( cudaEventRecord(startEvent, 0) );
    for (int i = 0; i < NUM_REPS; i++)
       copy<<<dimGrid, dimBlock>>>(d_tdata, d_idata);
    for (int i = 0; i < NUM_REPS; i++)
       copy<<<dimGrid, dimBlock>>>(d_idata, d_tdata);
    checkCuda( cudaEventRecord(stopEvent, 0) );
    checkCuda( cudaEventSynchronize(stopEvent) );
    checkCuda( cudaEventElapsedTime(&ms, startEvent, stopEvent) );
    checkCuda( cudaMemcpy(h_cdata, d_tdata, mem_size, cudaMemcpyDeviceToHost) );
    postprocess(h_idata, h_cdata, nx*ny, ms);

    // -------------
    // copySharedMem
    // -------------
    printf("%25s", "shared memory copy");
    checkCuda( cudaMemset(d_tdata, 0, mem_size) );
    // warm up
    copySharedMem<<<dimGrid, dimBlock>>>(d_tdata, d_idata);
    checkCuda( cudaEventRecord(startEvent, 0) );
    for (int i = 0; i < NUM_REPS; i++)
       copySharedMem<<<dimGrid, dimBlock>>>(d_tdata, d_idata);
    for (int i = 0; i < NUM_REPS; i++)
       copySharedMem<<<dimGrid, dimBlock>>>(d_idata, d_tdata);
    checkCuda( cudaEventRecord(stopEvent, 0) );
    checkCuda( cudaEventSynchronize(stopEvent) );
    checkCuda( cudaEventElapsedTime(&ms, startEvent, stopEvent) );
    checkCuda( cudaMemcpy(h_cdata, d_tdata, mem_size, cudaMemcpyDeviceToHost) );
    postprocess(h_idata, h_cdata, nx * ny, ms);

    // --------------
    // transposeNaive
    // --------------
    printf("%25s", "naive transpose");
    checkCuda( cudaMemset(d_tdata, 0, mem_size) );
    // warmup
    transposeNaive<<<dimGrid, dimBlock>>>(d_tdata, d_idata);
    checkCuda( cudaEventRecord(startEvent, 0) );
    for (int i = 0; i < NUM_REPS; i++)
       transposeNaive<<<dimGrid, dimBlock>>>(d_tdata, d_idata);
    for (int i = 0; i < NUM_REPS; i++)
       transposeNaive<<<dimGrid, dimBlock>>>(d_idata, d_tdata);
    checkCuda( cudaEventRecord(stopEvent, 0) );
    checkCuda( cudaEventSynchronize(stopEvent) );
    checkCuda( cudaEventElapsedTime(&ms, startEvent, stopEvent) );
    checkCuda( cudaMemcpy(h_tdata, d_tdata, mem_size, cudaMemcpyDeviceToHost) );
    postprocess(gold, h_tdata, nx * ny, ms);

    // ------------------
    // transposeCoalesced
    // ------------------
    printf("%25s", "coalesced transpose");
    checkCuda( cudaMemset(d_tdata, 0, mem_size) );
    // warmup
    transposeCoalesced<<<dimGrid, dimBlock>>>(d_tdata, d_idata);
    checkCuda( cudaEventRecord(startEvent, 0) );
    for (int i = 0; i < NUM_REPS; i++)
       transposeCoalesced<<<dimGrid, dimBlock>>>(d_tdata, d_idata);
    for (int i = 0; i < NUM_REPS; i++)
       transposeCoalesced<<<dimGrid, dimBlock>>>(d_idata, d_tdata);
    checkCuda( cudaEventRecord(stopEvent, 0) );
    checkCuda( cudaEventSynchronize(stopEvent) );
    checkCuda( cudaEventElapsedTime(&ms, startEvent, stopEvent) );
    checkCuda( cudaMemcpy(h_tdata, d_tdata, mem_size, cudaMemcpyDeviceToHost) );
    postprocess(gold, h_tdata, nx * ny, ms);

    // ------------------------
    // transposeNoBankConflicts
    // ------------------------
    printf("%25s", "conflict-free transpose");
    checkCuda( cudaMemset(d_tdata, 0, mem_size) );
    // warmup
    transposeNoBankConflicts<<<dimGrid, dimBlock>>>(d_tdata, d_idata);
    checkCuda( cudaEventRecord(startEvent, 0) );
    for (int i = 0; i < NUM_REPS; i++)
       transposeNoBankConflicts<<<dimGrid, dimBlock>>>(d_tdata, d_idata);
    for (int i = 0; i < NUM_REPS; i++)
       transposeNoBankConflicts<<<dimGrid, dimBlock>>>(d_idata, d_tdata);
    checkCuda( cudaEventRecord(stopEvent, 0) );
    checkCuda( cudaEventSynchronize(stopEvent) );
    checkCuda( cudaEventElapsedTime(&ms, startEvent, stopEvent) );
    checkCuda( cudaMemcpy(h_tdata, d_tdata, mem_size, cudaMemcpyDeviceToHost) );
    postprocess(gold, h_tdata, nx * ny, ms);

  error_exit:
    // cleanup
    checkCuda( cudaEventDestroy(startEvent) );
    checkCuda( cudaEventDestroy(stopEvent) );
    checkCuda( cudaFree(d_tdata) );
    checkCuda( cudaFree(d_idata) );
    free(h_idata);
    free(h_tdata);
    free(h_cdata);
    free(gold);

}

int main(int argc, char **argv)
{
  double size = 0.0;
  long long row_and_col_size = 20000;
  size_t free = 0, total = 0;  
  cudaMemGetInfo(&free,&total);
  double used_memory_in_GB = (total- free)/1024./1024./1024.;
  while(size < 4.096 - used_memory_in_GB)
  {
    size = 2*(row_and_col_size*row_and_col_size*4)/1024./1024./1024.;
    printf("\n");
    printf("--------------------------------------------\n");
    printf("Allocated GPU memory size in GB: %f\n",size);
    printf("Used GPU memory by other applications in GB: %f\n",used_memory_in_GB);
    printf("--------------------------------------------\n");
    bandwidth_test(row_and_col_size, argc, argv);
    row_and_col_size+=256;
  }

}
